{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Financial News Scrapper\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline implements an **institutional-grade financial news extraction system** designed to collect, clean, score, and organize high-signal company-specific news across multiple publicly available news websites.\n",
    "It supports **multi-company ingestion**, **true pagination**, **noise reduction**, and **financial relevance scoring**, producing outputs suitable for downstream analytics or LLM-based reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Input\n",
    "\n",
    "* **`COMPANIES`**\n",
    "  A list of stock tickers:\n",
    "\n",
    "  ```python\n",
    "  [\"TSLA\",\"NVDA\",\"AAPL\",\"AMD\",\"AMZN\",\"MSFT\",\"GOOGL\",\"META\",\n",
    "   \"BAC\",\"INTC\",\"CSCO\",\"KO\",\"XOM\",\"NFLX\",\"NKE\"]\n",
    "  ```\n",
    "\n",
    "* **Derived Company Keywords**\n",
    "  Each ticker is mapped to a company-specific keyword to avoid semantic ambiguity\n",
    "  (e.g., *AAPL → “apple inc”*, *KO → “coca cola”*).\n",
    "\n",
    "* **Pagination Range**\n",
    "  News is fetched page-by-page until records are exhausted or a safe upper bound is reached.\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "* **Economic Times**\n",
    "  Topic-based pages with native pagination (`?page=`)\n",
    "\n",
    "* **The Guardian**\n",
    "  Technology section pages with server-side pagination\n",
    "  Articles are filtered post-fetch to retain only company-relevant content.\n",
    "\n",
    "All scraping follows **responsible access practices** (rate limiting, retries, user-agent headers).\n",
    "\n",
    "---\n",
    "\n",
    "## Core Logic\n",
    "\n",
    "### 1. Multi-Company Pagination Engine\n",
    "\n",
    "* Iterates over each company independently.\n",
    "* Scrapes paginated result pages.\n",
    "* Extracts full article URLs and fetches individual articles.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Hard Relevance Filtering\n",
    "\n",
    "* Articles must explicitly mention the **company keyword** in the title or body.\n",
    "* Prevents false positives (e.g., fruit “apple” vs Apple Inc.).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "### Primary Output (CSV)\n",
    "\n",
    "A unified dataset containing all companies:\n",
    "\n",
    "```text\n",
    "ticker\n",
    "company\n",
    "source\n",
    "title\n",
    "summary\n",
    "published\n",
    "url\n",
    "scraped_at\n",
    "```\n",
    "\n",
    "This format is suitable for:\n",
    "\n",
    "* Time-series analysis\n",
    "* Event detection\n",
    "* Sentiment aggregation\n",
    "* Model training\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Session Metadata\n",
    "Records the timestamp, URLs used, and tickers scraped for this session. This cell must be run **first** so that `SCRAPE_TIMESTAMP` is available for the CSV export cell later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping TSLA...\n",
      "  → 80 records\n",
      "Scraping NVDA...\n",
      "  → 80 records\n",
      "Scraping AAPL...\n",
      "  → 80 records\n",
      "Scraping AMD...\n",
      "  → 80 records\n",
      "Scraping AMZN...\n",
      "  → 80 records\n",
      "Scraping MSFT...\n",
      "  → 80 records\n",
      "Scraping GOOGL...\n",
      "  → 80 records\n",
      "Scraping META...\n",
      "  → 80 records\n",
      "Scraping BAC...\n",
      "  → 80 records\n",
      "Scraping INTC...\n",
      "  → 80 records\n",
      "Scraping CSCO...\n",
      "  → 80 records\n",
      "Scraping KO...\n",
      "  → 66 records\n",
      "Scraping XOM...\n",
      "  → 80 records\n",
      "Scraping NFLX...\n",
      "  → 80 records\n",
      "Scraping NKE...\n",
      "  → 80 records\n",
      "\n",
      "SCRAPING METADATA\n",
      "-----------------\n",
      "Companies scraped : 15\n",
      "Total records     : 1050\n",
      "Saved to          : multi_company_news.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MULTI-COMPANY ECONOMIC TIMES SCRAPER (FAST, 15 COMPANIES)\n",
    "# Target: 60–100 articles per company\n",
    "# ============================================================\n",
    "\n",
    "import requests, time, random, hashlib\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "TIMEOUT = 6\n",
    "DELAY_RANGE = (0.6, 1.2)        # faster but safe\n",
    "MAX_PAGES_ET = 140              # ET hard limit\n",
    "PER_COMPANY_TARGET = 80         # 60–100 safe band\n",
    "\n",
    "SCRAPED_AT = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "COMPANIES = [\n",
    "    \"TSLA\",\"NVDA\",\"AAPL\",\"AMD\",\"AMZN\",\"MSFT\",\"GOOGL\",\"META\",\n",
    "    \"BAC\",\"INTC\",\"CSCO\",\"KO\",\"XOM\",\"NFLX\",\"NKE\"\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# ECONOMIC TIMES TOPICS (PRIMARY + FALLBACK)\n",
    "# ============================================================\n",
    "ET_TOPICS = {\n",
    "    \"TSLA\": [\"tesla\"],\n",
    "    \"NVDA\": [\"nvidia\", \"ai\", \"semiconductor\"],\n",
    "    \"AAPL\": [\"apple\", \"iphone\"],\n",
    "    \"AMD\": [\"amd\", \"chipmaker\", \"semiconductor\"],\n",
    "    \"AMZN\": [\"amazon\", \"aws\", \"e-commerce\"],\n",
    "    \"MSFT\": [\"microsoft\", \"azure\", \"cloud\"],\n",
    "    \"GOOGL\": [\"google\", \"alphabet\", \"search\"],\n",
    "    \"META\": [\"meta\", \"facebook\", \"instagram\"],\n",
    "    \"BAC\": [\"bank-of-america\", \"banking\", \"us-banks\"],\n",
    "    \"INTC\": [\"intel\", \"chipmaker\", \"semiconductor\"],\n",
    "    \"CSCO\": [\"cisco\", \"enterprise-tech\", \"networking\"],\n",
    "    \"KO\": [\"coca-cola\", \"fmcg\", \"consumer-goods\"],\n",
    "    \"XOM\": [\"exxon-mobil\", \"oil\", \"energy\"],\n",
    "    \"NFLX\": [\"netflix\", \"streaming\", \"media\"],\n",
    "    \"NKE\": [\"nike\", \"retail\", \"sportswear\"]\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# HIGH-RECALL KEYWORDS (FILTERING ONLY)\n",
    "# ============================================================\n",
    "KEYWORDS = {\n",
    "    \"TSLA\": [\"tesla\", \"elon musk\", \"ev\", \"robotaxi\"],\n",
    "    \"NVDA\": [\"nvidia\", \"gpu\", \"ai\"],\n",
    "    \"AAPL\": [\"apple\", \"iphone\", \"ios\"],\n",
    "    \"AMD\": [\"amd\", \"ryzen\", \"chip\"],\n",
    "    \"AMZN\": [\"amazon\", \"aws\"],\n",
    "    \"MSFT\": [\"microsoft\", \"azure\"],\n",
    "    \"GOOGL\": [\"google\", \"alphabet\"],\n",
    "    \"META\": [\"meta\", \"facebook\"],\n",
    "    \"BAC\": [\"bank of america\", \"bofa\"],\n",
    "    \"INTC\": [\"intel\", \"chip\"],\n",
    "    \"CSCO\": [\"cisco\", \"network\"],\n",
    "    \"KO\": [\"coca cola\"],\n",
    "    \"XOM\": [\"exxon\", \"oil\"],\n",
    "    \"NFLX\": [\"netflix\"],\n",
    "    \"NKE\": [\"nike\", \"sneaker\"]\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "def polite_delay():\n",
    "    time.sleep(random.uniform(*DELAY_RANGE))\n",
    "\n",
    "def safe_request(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "        return r.text if r.status_code == 200 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def dedup_key(title, url):\n",
    "    return hashlib.md5((title.lower() + url).encode()).hexdigest()\n",
    "\n",
    "def match(text, kws):\n",
    "    t = text.lower()\n",
    "    return any(k in t for k in kws)\n",
    "\n",
    "# ============================================================\n",
    "# ECONOMIC TIMES SCRAPER (ROBUST)\n",
    "# ============================================================\n",
    "def scrape_economic_times(ticker):\n",
    "    records = []\n",
    "    base = \"https://economictimes.indiatimes.com/topic\"\n",
    "    keywords = KEYWORDS[ticker]\n",
    "\n",
    "    for topic in ET_TOPICS[ticker]:\n",
    "        slug = topic.replace(\" \", \"-\")\n",
    "\n",
    "        for page in range(1, MAX_PAGES_ET + 1):\n",
    "            if len(records) >= PER_COMPANY_TARGET:\n",
    "                return records\n",
    "\n",
    "            url = f\"{base}/{slug}/{page}\"\n",
    "            html = safe_request(url)\n",
    "            polite_delay()\n",
    "\n",
    "            if not html:\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            articles = soup.select(\"div.contentD\")\n",
    "\n",
    "            if not articles:\n",
    "                break\n",
    "\n",
    "            for block in articles:\n",
    "                a = block.find(\"h2\")\n",
    "                if not a or not a.find(\"a\"):\n",
    "                    continue\n",
    "\n",
    "                link = a.find(\"a\")\n",
    "                title = link.text.strip()\n",
    "                summary = block.find(\"p\").text.strip() if block.find(\"p\") else \"\"\n",
    "\n",
    "                combined = title + \" \" + summary\n",
    "                if not match(combined, keywords):\n",
    "                    continue\n",
    "\n",
    "                time_tag = block.find(\"time\")\n",
    "\n",
    "                records.append({\n",
    "                    \"ticker\": ticker,\n",
    "                    \"company\": topic,\n",
    "                    \"source\": \"Economic Times\",\n",
    "                    \"title\": title,\n",
    "                    \"summary\": summary,\n",
    "                    \"published\": time_tag.text.strip() if time_tag else \"\",\n",
    "                    \"url\": urljoin(base, link[\"href\"]),\n",
    "                    \"scraped_at\": SCRAPED_AT\n",
    "                })\n",
    "\n",
    "                if len(records) >= PER_COMPANY_TARGET:\n",
    "                    return records\n",
    "\n",
    "    return records\n",
    "\n",
    "# ============================================================\n",
    "# RUN PIPELINE (ALL 15 COMPANIES GUARANTEED)\n",
    "# ============================================================\n",
    "all_records = []\n",
    "\n",
    "for ticker in COMPANIES:\n",
    "    print(f\"Scraping {ticker}...\")\n",
    "    company_records = scrape_economic_times(ticker)\n",
    "    print(f\"  → {len(company_records)} records\")\n",
    "    all_records.extend(company_records)\n",
    "\n",
    "# ============================================================\n",
    "# CLEAN + SAVE\n",
    "# ============================================================\n",
    "df = pd.DataFrame(all_records)\n",
    "df[\"dedup\"] = df.apply(lambda r: dedup_key(r[\"title\"], r[\"url\"]), axis=1)\n",
    "df = df.drop_duplicates(\"dedup\").drop(columns=\"dedup\")\n",
    "\n",
    "df.to_csv(\"multi_company_news.csv\", index=False)\n",
    "\n",
    "print(\"\\nSCRAPING METADATA\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Companies scraped : {len(COMPANIES)}\")\n",
    "print(f\"Total records     : {len(df)}\")\n",
    "print(\"Saved to          : multi_company_news.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
