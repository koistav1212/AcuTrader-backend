{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial News Extraction Pipeline\n",
        "\n",
        "## Overview\n",
        "This notebook contains an institutional-grade news fetcher designed to extract high-signal financial news for a set of companies over a specific period (like 30 days), categorise them, and output a structured JSON.\n",
        "\n",
        "## Input\n",
        "- `tickers`: A list of 25 companies, e.g., `[\"TSLA\", \"NVDA\", \"AAPL\", ...]`\n",
        "- `days`: Number of days to fetch news for (30 days by default).\n",
        "\n",
        "## Logic\n",
        "1. **Financial Intent Classifier**: Categorizes extracted news into targeted buckets (earnings, analyst, management, corporate, regulation, etc.)\n",
        "2. **Source Credibility Weighting**: Boosts scores for premium sources like Reuters, Bloomberg, WSJ.\n",
        "3. **Semantic Similarity Filter**: Filters out non-financial noise using local dense embeddings (`all-MiniLM-L6-v2`).\n",
        "4. **Categorization & Pagination**: Limits news per category to avoid spam and focuses only on high-quality articles.\n",
        "\n",
        "## Output\n",
        "A hierarchical JSON document containing:\n",
        "```json\n",
        "{\n",
        "   \"TICKER\": {\n",
        "      \"news_count\": 10,\n",
        "      \"30_day_news\": {\n",
        "         \"YYYY-MM-DD\": {\n",
        "             \"category_name\": [\n",
        "                  { \"title\": \"...\", \"summary\": \"...\", \"source\": \"...\" }\n",
        "             ]\n",
        "         }\n",
        "      }\n",
        "   }\n",
        "}\n",
        "```\n",
        "This JSON can directly be used to feed into an LLM context window to generate analytical summaries (.join(['probabilities', 'bull/bear case'])) per day or overall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic filter: ENABLED\n"
          ]
        }
      ],
      "source": [
        "import feedparser\n",
        "import requests\n",
        "import re\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import ssl\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import json\n",
        "\n",
        "# SSL Fix\n",
        "if hasattr(ssl, '_create_unverified_context'):\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# ============================================================\n",
        "# UPGRADE 3: SEMANTIC SIMILARITY FILTER\n",
        "# ============================================================\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    SEMANTIC_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    FINANCE_REFERENCE = \"earnings revenue guidance profit loss merger acquisition regulation lawsuit analyst rating upgrade downgrade CEO CFO quarterly results forecast dividend buyback IPO\"\n",
        "    REF_EMBEDDING = SEMANTIC_MODEL.encode(FINANCE_REFERENCE)\n",
        "    SEMANTIC_ENABLED = True\n",
        "    print(\"Semantic filter: ENABLED\")\n",
        "except ImportError:\n",
        "    SEMANTIC_ENABLED = False\n",
        "    print(\"Semantic filter: DISABLED (sentence-transformers not installed)\")\n",
        "\n",
        "# ============================================================\n",
        "# UPGRADE 1: FINANCIAL INTENT CLASSIFIER\n",
        "# ============================================================\n",
        "\n",
        "CATEGORY_RULES = {\n",
        "    \"earnings\": [\"earnings\", \"eps\", \"revenue\", \"quarter\", \"results\", \"guidance\", \"forecast\", \"profit\", \"loss\", \"beat\", \"miss\"],\n",
        "    \"analyst\": [\"upgrade\", \"downgrade\", \"price target\", \"rating\", \"initiated\", \"reiterate\", \"analyst\"],\n",
        "    \"management\": [\"ceo\", \"cfo\", \"board\", \"resigns\", \"appoints\", \"executive\", \"leadership\"],\n",
        "    \"corporate\": [\"acquisition\", \"merger\", \"buyback\", \"dividend\", \"split\", \"deal\", \"partnership\", \"expansion\"],\n",
        "    \"filing\": [\"13f\", \"stake\", \"holdings\", \"llc increases\", \"llc reduces\", \"management purchased\", \"increases position\", \n",
        "               \"reduces position\", \"institutional\", \"buys shares\", \"sells shares\", \"buys new shares\", \"new position in\"],\n",
        "    \"regulation\": [\"sec\", \"lawsuit\", \"settlement\", \"investigation\", \"fine\", \"penalty\", \"compliance\"],\n",
        "}\n",
        "\n",
        "# Categories to KEEP (high signal)\n",
        "KEEP_CATEGORIES = [\"earnings\", \"analyst\", \"management\", \"corporate\", \"regulation\"]\n",
        "\n",
        "# Categories to DISCARD (low signal, high noise)\n",
        "DISCARD_CATEGORIES = [\"filing\"]\n",
        "\n",
        "# Ownership spam patterns (HARD BLOCK)\n",
        "OWNERSHIP_SPAM_PATTERNS = [\n",
        "    \"buys shares\", \"sells shares\", \"buys new shares\", \"increases holdings\",\n",
        "    \"reduces holdings\", \"new position in\", \"llc buys\", \"llc sells\",\n",
        "    \"advisors buys\", \"advisors sells\", \"management increases\", \"management reduces\",\n",
        "    \"grows stock holdings\", \"raises stock position\", \"sells 4,\", \"sells 3,\", \"sells 2,\", \"sells 1,\"\n",
        "]\n",
        "\n",
        "def is_ownership_spam(text):\n",
        "    \"\"\"Returns True if article is ownership/institutional filing spam.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    return any(pattern in text_lower for pattern in OWNERSHIP_SPAM_PATTERNS)\n",
        "\n",
        "def classify_article(text):\n",
        "    \"\"\"\n",
        "    Classifies article into financial categories.\n",
        "    Returns the primary category or 'general'.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    scores = {}\n",
        "    \n",
        "    for category, keywords in CATEGORY_RULES.items():\n",
        "        score = sum(1 for kw in keywords if kw in text_lower)\n",
        "        if score > 0:\n",
        "            scores[category] = score\n",
        "    \n",
        "    if not scores:\n",
        "        return \"general\"\n",
        "    \n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "# ============================================================\n",
        "# UPGRADE 2: SOURCE CREDIBILITY WEIGHTING\n",
        "# ============================================================\n",
        "\n",
        "SOURCE_WEIGHT = {\n",
        "    \"reuters\": 5,\n",
        "    \"wsj\": 5,\n",
        "    \"ft.com\": 5,\n",
        "    \"bloomberg\": 5,\n",
        "    \"cnbc\": 4,\n",
        "    \"seekingalpha\": 3,\n",
        "    \"yahoo\": 3,\n",
        "    \"benzinga\": 2,\n",
        "    \"marketwatch\": 2,\n",
        "    \"nasdaq\": 2,\n",
        "    \"stocktwits\": 0,\n",
        "    \"google\": 1,  # Google News aggregates, variable quality\n",
        "}\n",
        "\n",
        "def get_source_weight(link, source_name):\n",
        "    \"\"\"Returns credibility weight based on source.\"\"\"\n",
        "    link_lower = link.lower()\n",
        "    source_lower = source_name.lower()\n",
        "    \n",
        "    for source, weight in SOURCE_WEIGHT.items():\n",
        "        if source in link_lower or source in source_lower:\n",
        "            return weight\n",
        "    \n",
        "    return 1  # Default weight\n",
        "\n",
        "# ============================================================\n",
        "# UPGRADE 4: STRONGER NOISE BLACKLIST\n",
        "# ============================================================\n",
        "\n",
        "NOISE_KEYWORDS = [\n",
        "    # Weather\n",
        "    \"weather\", \"storm\", \"hurricane\", \"flood\", \"tornado\", \"freeze\", \"snow\",\n",
        "    # Violence/Crime\n",
        "    \"shooting\", \"murder\", \"crime\", \"arrest\", \"police\",\n",
        "    # War/Military\n",
        "    \"war\", \"military\", \"invasion\", \"troops\", \"missile\", \"ukraine\", \"russia\",\n",
        "    # Politics (unless directly business-related)\n",
        "    \"election\", \"vote\", \"congress\", \"senate\", \"democrat\", \"republican\", \"trump\", \"biden\",\n",
        "    # Sports/Entertainment\n",
        "    \"sports\", \"game\", \"celebrity\", \"movie\", \"concert\", \"nfl\", \"nba\",\n",
        "    # Travel disruptions\n",
        "    \"airline delays\", \"flight cancel\", \"airport\",\n",
        "    # Crypto-only noise (unless it's about the company's crypto strategy)\n",
        "    \"bitcoin price\", \"crypto crash\", \"meme coin\",\n",
        "]\n",
        "\n",
        "def has_noise(text):\n",
        "    \"\"\"Returns True if article contains noise keywords.\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    noise_count = sum(1 for kw in NOISE_KEYWORDS if kw in text_lower)\n",
        "    return noise_count >= 2  # Allow 1 mention, block 2+\n",
        "\n",
        "# ============================================================\n",
        "# LAYER 1: SOURCE SEPARATION (Tier A = ticker-specific only)\n",
        "# ============================================================\n",
        "\n",
        "def get_rss_feeds(ticker):\n",
        "    \"\"\"\n",
        "    Returns ONLY high-quality ticker-specific feeds.\n",
        "    Removed all generic macro noise sources.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        # Tier A: Ticker-specific (HIGH RELEVANCE)\n",
        "        f\"https://news.google.com/rss/search?q={ticker}+stock&hl=en-US&gl=US&ceid=US:en\",\n",
        "        f\"https://feeds.finance.yahoo.com/rss/2.0/headline?s={ticker}\",\n",
        "        f\"https://www.nasdaq.com/feed/rssoutbound?symbol={ticker}\",\n",
        "        f\"https://stocktwits.com/symbol/{ticker}.rss\",\n",
        "        f\"https://seekingalpha.com/api/sa/combined/{ticker}.xml\",\n",
        "        \n",
        "        # Tier B: Quality sources (will be filtered by ticker presence)\n",
        "        \"https://feeds.benzinga.com/benzinga\",\n",
        "        \"https://seekingalpha.com/market_currents.xml\",\n",
        "    ]\n",
        "\n",
        "# ============================================================\n",
        "# COMPANY NAME LOOKUP\n",
        "# ============================================================\n",
        "\n",
        "_company_cache = {}\n",
        "\n",
        "def get_company_info(ticker):\n",
        "    \"\"\"Returns company name and related keywords for filtering.\"\"\"\n",
        "    if ticker in _company_cache:\n",
        "        return _company_cache[ticker]\n",
        "    \n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        info = stock.info\n",
        "        \n",
        "        company_name = info.get('shortName', '') or info.get('longName', '')\n",
        "        \n",
        "        keywords = [ticker.lower()]\n",
        "        \n",
        "        if company_name:\n",
        "            keywords.append(company_name.lower())\n",
        "            for word in company_name.split():\n",
        "                if len(word) > 3:\n",
        "                    keywords.append(word.lower())\n",
        "        \n",
        "        _company_cache[ticker] = keywords\n",
        "        return keywords\n",
        "        \n",
        "    except Exception:\n",
        "        _company_cache[ticker] = [ticker.lower()]\n",
        "        return [ticker.lower()]\n",
        "\n",
        "# ============================================================\n",
        "# HIGH IMPACT KEYWORDS\n",
        "# ============================================================\n",
        "\n",
        "HIGH_IMPACT_KEYWORDS = [\n",
        "    \"earnings\", \"revenue\", \"guidance\", \"quarter\", \"profit\", \"loss\",\n",
        "    \"upgrade\", \"downgrade\", \"beat\", \"miss\", \"forecast\", \"outlook\",\n",
        "    \"acquisition\", \"merger\", \"buyback\", \"dividend\", \"split\",\n",
        "    \"sec\", \"filing\", \"lawsuit\", \"settlement\", \"investigation\",\n",
        "    \"ceo\", \"cfo\", \"executive\", \"board\", \"analyst\"\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# RELEVANCE FILTER (HARD FILTER)\n",
        "# ============================================================\n",
        "\n",
        "def is_relevant(article, ticker, company_keywords):\n",
        "    \"\"\"HARD FILTER: Returns True only if article is company-specific.\"\"\"\n",
        "    text = (article[\"title\"] + \" \" + article.get(\"summary\", \"\")).lower()\n",
        "    \n",
        "    for kw in company_keywords:\n",
        "        if kw in text:\n",
        "            return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "# ============================================================\n",
        "# UPGRADE 3: SEMANTIC SIMILARITY SCORE\n",
        "# ============================================================\n",
        "\n",
        "def semantic_score(text):\n",
        "    \"\"\"Returns semantic similarity to financial reference text.\"\"\"\n",
        "    if not SEMANTIC_ENABLED:\n",
        "        return 1.0  # Bypass if not available\n",
        "    \n",
        "    try:\n",
        "        emb = SEMANTIC_MODEL.encode(text[:500])  # Limit text length\n",
        "        score = util.cos_sim(emb, REF_EMBEDDING).item()\n",
        "        return score\n",
        "    except Exception:\n",
        "        return 0.5\n",
        "\n",
        "# ============================================================\n",
        "# COMPLETE SCORING FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def score_article(article, ticker, company_keywords):\n",
        "    \"\"\"\n",
        "    PRO scoring with all upgrades:\n",
        "    - Ticker/company presence\n",
        "    - High-impact keywords\n",
        "    - Source credibility\n",
        "    - Semantic similarity\n",
        "    - Category bonuses\n",
        "    - Noise penalties\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    title = article[\"title\"].lower()\n",
        "    text = (title + \" \" + article.get(\"summary\", \"\")).lower()\n",
        "    link = article.get(\"link\", \"\").lower()\n",
        "    source = article.get(\"source\", \"\")\n",
        "    \n",
        "    # === BASE SCORING ===\n",
        "    \n",
        "    # Ticker in title: +10 (strongest signal)\n",
        "    if ticker.lower() in title:\n",
        "        score += 10\n",
        "    elif ticker.lower() in text:\n",
        "        score += 5\n",
        "    \n",
        "    # Company name match: +6\n",
        "    for kw in company_keywords:\n",
        "        if kw != ticker.lower() and kw in text:\n",
        "            score += 6\n",
        "            break\n",
        "    \n",
        "    # High-impact keywords: +2 each (max 8)\n",
        "    impact_count = sum(1 for k in HIGH_IMPACT_KEYWORDS if k in text)\n",
        "    score += min(impact_count * 2, 8)\n",
        "    \n",
        "    # === UPGRADE 2: Source credibility ===\n",
        "    score += get_source_weight(link, source)\n",
        "    \n",
        "    # === UPGRADE 3: Semantic similarity bonus ===\n",
        "    if SEMANTIC_ENABLED:\n",
        "        sem_score = semantic_score(text)\n",
        "        if sem_score > 0.35:\n",
        "            score += 5\n",
        "        elif sem_score > 0.25:\n",
        "            score += 2\n",
        "        elif sem_score < 0.15:\n",
        "            score -= 5  # Penalize non-financial content\n",
        "    \n",
        "    # === UPGRADE 1: Category bonuses ===\n",
        "    category = classify_article(text)\n",
        "    article['_category'] = category  # Store for quota system\n",
        "    \n",
        "    if category in [\"earnings\", \"analyst\"]:\n",
        "        score += 4\n",
        "    elif category in [\"management\", \"corporate\"]:\n",
        "        score += 3\n",
        "    elif category in [\"regulation\"]:\n",
        "        score += 2\n",
        "    elif category in [\"filing\"]:\n",
        "        score -= 8  # Heavy penalty for ownership spam\n",
        "    \n",
        "    # === UPGRADE 4: Noise penalties ===\n",
        "    if has_noise(text):\n",
        "        score -= 10\n",
        "    \n",
        "    return score\n",
        "\n",
        "# ============================================================\n",
        "# DEDUPLICATION\n",
        "# ============================================================\n",
        "\n",
        "def deduplicate_articles(articles):\n",
        "    \"\"\"Deduplicates articles based on normalized title similarity.\"\"\"\n",
        "    unique_articles = []\n",
        "    seen_normalized_titles = set()\n",
        "    \n",
        "    for article in articles:\n",
        "        norm_title = re.sub(r'\\W+', '', article['title'].lower())\n",
        "        \n",
        "        # More aggressive dedup: first 50 chars\n",
        "        short_key = norm_title[:50]\n",
        "        \n",
        "        if short_key in seen_normalized_titles:\n",
        "            continue\n",
        "            \n",
        "        seen_normalized_titles.add(short_key)\n",
        "        unique_articles.append(article)\n",
        "        \n",
        "    return unique_articles\n",
        "\n",
        "# ============================================================\n",
        "# UPGRADE 5: CATEGORY QUOTAS\n",
        "# ============================================================\n",
        "\n",
        "def apply_category_quotas(articles, quota_per_category=5, total_limit=50):\n",
        "    \"\"\"\n",
        "    Returns balanced articles with max N per category.\n",
        "    Ensures diverse summary (Bloomberg style).\n",
        "    \"\"\"\n",
        "    bucket = defaultdict(list)\n",
        "    \n",
        "    for article in articles:\n",
        "        category = article.get('_category', 'general')\n",
        "        bucket[category].append(article)\n",
        "    \n",
        "    final = []\n",
        "    \n",
        "    # Priority order\n",
        "    priority_categories = [\"earnings\", \"analyst\", \"corporate\", \"management\", \"regulation\", \"general\"]\n",
        "    \n",
        "    for cat in priority_categories:\n",
        "        if cat in bucket:\n",
        "            final.extend(bucket[cat][:quota_per_category])\n",
        "    \n",
        "    # Fill remaining with any leftover high-scoring articles\n",
        "    if len(final) < total_limit:\n",
        "        all_remaining = [a for a in articles if a not in final]\n",
        "        final.extend(all_remaining[:total_limit - len(final)])\n",
        "    \n",
        "    return final[:total_limit]\n",
        "\n",
        "# ============================================================\n",
        "# MAIN FETCH FUNCTION (with all 5 upgrades)\n",
        "# ============================================================\n",
        "\n",
        "def fetch_news_data(ticker, days=30):\n",
        "    \"\"\"\n",
        "    INSTITUTIONAL-GRADE news fetcher with 5 upgrades:\n",
        "    1. Financial intent classifier\n",
        "    2. Source credibility weighting\n",
        "    3. Semantic similarity filter\n",
        "    4. Stronger noise blacklist\n",
        "    5. Category quotas\n",
        "    \"\"\"\n",
        "    feeds = get_rss_feeds(ticker)\n",
        "    cutoff_date = datetime.now() - timedelta(days=days)\n",
        "    \n",
        "    company_keywords = get_company_info(ticker)\n",
        "    \n",
        "    raw_articles = []\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
        "    }\n",
        "\n",
        "    def fetch_single_feed(rss_url):\n",
        "        articles = []\n",
        "        try:\n",
        "            response = requests.get(rss_url, headers=headers, timeout=5)\n",
        "            if response.status_code != 200:\n",
        "                return []\n",
        "            \n",
        "            feed = feedparser.parse(response.content)\n",
        "\n",
        "            for entry in feed.entries:\n",
        "                published_dt = None\n",
        "                if hasattr(entry, 'published_parsed') and entry.published_parsed:\n",
        "                    published_dt = datetime.fromtimestamp(time.mktime(entry.published_parsed))\n",
        "                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:\n",
        "                    published_dt = datetime.fromtimestamp(time.mktime(entry.updated_parsed))\n",
        "                \n",
        "                if not published_dt or published_dt < cutoff_date:\n",
        "                    continue\n",
        "\n",
        "                title = entry.title if hasattr(entry, 'title') else ''\n",
        "                link = entry.link if hasattr(entry, 'link') else ''\n",
        "                summary = entry.summary if hasattr(entry, 'summary') else ''\n",
        "                \n",
        "                def clean_text(text):\n",
        "                    if not text: return \"\"\n",
        "                    text = re.sub(r'<[^>]+>', '', text)\n",
        "                    text = text.replace(\"&nbsp;\", \" \").replace(\"&amp;\", \"&\")\n",
        "                    return \" \".join(text.split())\n",
        "\n",
        "                title = clean_text(title)\n",
        "                summary = clean_text(summary)\n",
        "                \n",
        "                if len(summary) < 20:\n",
        "                    summary = title\n",
        "                    \n",
        "                source_title = getattr(feed.feed, 'title', 'Unknown')\n",
        "\n",
        "                articles.append({\n",
        "                    'title': title,\n",
        "                    'link': link,\n",
        "                    'published': published_dt.strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'summary': summary,\n",
        "                    'source': source_title\n",
        "                })\n",
        "        except Exception:\n",
        "            pass\n",
        "            \n",
        "        return articles\n",
        "\n",
        "    # Parallel fetch\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        future_to_url = {executor.submit(fetch_single_feed, url): url for url in feeds}\n",
        "        for future in as_completed(future_to_url):\n",
        "            try:\n",
        "                data = future.result()\n",
        "                raw_articles.extend(data)\n",
        "            except Exception:\n",
        "                pass\n",
        "    \n",
        "    # Deduplicate\n",
        "    articles = deduplicate_articles(raw_articles)\n",
        "    \n",
        "    # Hard ticker filter\n",
        "    articles = [a for a in articles if is_relevant(a, ticker, company_keywords)]\n",
        "    \n",
        "    # Remove noise\n",
        "    articles = [a for a in articles if not has_noise(a[\"title\"] + \" \" + a.get(\"summary\", \"\"))]\n",
        "    \n",
        "    # Score and rank\n",
        "    for article in articles:\n",
        "        article['_score'] = score_article(article, ticker, company_keywords)\n",
        "    \n",
        "    articles.sort(key=lambda x: x['_score'], reverse=True)\n",
        "    \n",
        "    # Apply category quotas\n",
        "    top_articles = apply_category_quotas(articles, quota_per_category=5, total_limit=50)\n",
        "    \n",
        "    return top_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating the Extracted Output\n",
        "The target output is a nested dictionary mapping each company to its 30-day news breakdown. We iterate over the 25 requested symbols and collect the data into the requested shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching data for TSLA...\n",
            "Fetching data for NVDA...\n",
            "Fetching data for AAPL...\n",
            "Fetching data for AMD...\n",
            "Fetching data for AMZN...\n",
            "Fetching data for MSFT...\n",
            "Fetching data for GOOGL...\n",
            "Fetching data for META...\n",
            "Fetching data for BAC...\n",
            "Fetching data for INTC...\n",
            "Fetching data for CSCO...\n",
            "Fetching data for KO...\n",
            "Fetching data for XOM...\n",
            "Fetching data for NFLX...\n",
            "Fetching data for NKE...\n",
            "Processed extraction for 15 companies. Saved to companies_30_day_categorical_news.json\n"
          ]
        }
      ],
      "source": [
        "COMPANIES = [\n",
        "    \"TSLA\", \"NVDA\", \"AAPL\", \"AMD\",  \"AMZN\", \"MSFT\", \"GOOGL\", \"META\", \n",
        "    \"BAC\", \"INTC\", \"CSCO\", \"KO\", \"XOM\", \n",
        "     \"NFLX\", \"NKE\",\n",
        "]\n",
        "\n",
        "def extract_daily_categorical_news(tickers, days=30):\n",
        "    all_companies_data = {}\n",
        "    \n",
        "    for ticker in tickers:\n",
        "        print(f\"Fetching data for {ticker}...\")\n",
        "        articles = fetch_news_data(ticker, days=days)\n",
        "        \n",
        "        # Structure: date -> category -> list of articles\n",
        "        daily_news = defaultdict(lambda: defaultdict(list))\n",
        "        \n",
        "        for idx, a in enumerate(articles):\n",
        "            # publish format: YYYY-MM-DD HH:MM:SS\n",
        "            pub_date = a['published'].split(' ')[0]\n",
        "            \n",
        "            # Re-fetch category if we want, or fall back to '_category'. \n",
        "            # Our implementation retains `_category`.\n",
        "            category = a.get('_category', 'general')\n",
        "            \n",
        "            # Store simplified representation for JSON output\n",
        "            daily_news[pub_date][category].append({\n",
        "        \n",
        "                \"summary\": a['summary'],\n",
        "                \"source\": a['source'],\n",
        "               \n",
        "            })\n",
        "\n",
        "        all_companies_data[ticker] = {\n",
        "            \"news_count\": len(articles),\n",
        "            \"30_day_news\": { date: dict(categories) for date, categories in daily_news.items() }\n",
        "        }\n",
        "        \n",
        "    return all_companies_data\n",
        "\n",
        "# Set list for a quick test run to first 2 companies, to fetch actual complete output use `COMPANIES` list.\n",
        "output_data = extract_daily_categorical_news(COMPANIES, days=30)\n",
        "\n",
        "# Print out the result JSON as requested\n",
        "# To avoid massive console logs, we write to a file first. We then print an excerpt.\n",
        "with open(\"companies_30_day_categorical_news.json\", \"w\") as f:\n",
        "    json.dump(output_data, f, indent=4)\n",
        "    \n",
        "print(f\"Processed extraction for {len(COMPANIES)} companies. Saved to companies_30_day_categorical_news.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Text to Numerical Scoring & Ranking\n",
        "We will now take the extracted json output from above, and transform the text (title + summary) into numerical representations using `all-MiniLM-L6-v2` (SentenceTransformer).\n",
        "We'll compute a relevance/impact score against predefined financial keywords, rank the daily news based on this numerical score, and store the updated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SentenceTransformer model...\n",
            "Loaded JSON data for scoring.\n",
            "Computing semantic scores and ranking...\n",
            "Scoring and ranking complete. Saved to companies_ranked_news.json\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# 1. Load the model\n",
        "# all-MiniLM-L6-v2 is an excellent balance of speed and performance for semantic similarity.\n",
        "print(\"Loading SentenceTransformer model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Define Reference Embeddings\n",
        "# We define what a 'High Impact' article looks like for stock movement.\n",
        "HIGH_IMPACT_PHRASES = [\n",
        "    \"Better than expected earnings revenue beat growth surge\",\n",
        "    \"Analyst upgrade price target increase buy rating\",\n",
        "    \"Major acquisition merger buyout new partnership\",\n",
        "    \"Unexpected CEO resignation leadership change\",\n",
        "    \"SEC investigation lawsuit regulatory penalty\"\n",
        "]\n",
        "reference_embeddings = model.encode(HIGH_IMPACT_PHRASES, convert_to_tensor=True)\n",
        "\n",
        "def compute_semantic_score( summary):\n",
        "    \"\"\"\n",
        "    Embeds the text and computes the maximum cosine similarity\n",
        "    against the predefined high-impact phrases.\n",
        "    \"\"\"\n",
        "    text = f\"{summary}\"\n",
        "    text_embedding = model.encode(text, convert_to_tensor=True)\n",
        "    \n",
        "    # Compute cosine similarities\n",
        "    cosine_scores = util.cos_sim(text_embedding, reference_embeddings)\n",
        "    \n",
        "    # Return the highest similarity score (0 to 1)\n",
        "    max_score = torch.max(cosine_scores).item()\n",
        "    return round(max_score, 4)\n",
        "\n",
        "# 3. Load the data (or use the variable from the previous cell)\n",
        "try:\n",
        "    with open(\"companies_30_day_categorical_news.json\", \"r\") as f:\n",
        "        news_data = json.load(f)\n",
        "    print(\"Loaded JSON data for scoring.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Using in-memory output_data for scoring.\")\n",
        "    news_data = output_data  # defined in previous cell\n",
        "\n",
        "def rank_and_score_news(data):\n",
        "    \"\"\"\n",
        "    Iterates through the structured JSON, computes semantic scores,\n",
        "    and sorts them descending per day per category.\n",
        "    \"\"\"\n",
        "    print(\"Computing semantic scores and ranking...\")\n",
        "    # Recreate a new dictionary to avoid mutating while iterating if needed,\n",
        "    # but we can also update in place.\n",
        "    for ticker, ticker_data in data.items():\n",
        "        # if \"14_day_news\" in ticker_data:\n",
        "        # Let's find the correct key dynamically (14_day_news or 30_day_news)\n",
        "        news_key = next((k for k in ticker_data.keys() if \"_news\" in k), None)\n",
        "        if not news_key:\n",
        "            continue\n",
        "            \n",
        "        for date, categories in ticker_data[news_key].items():\n",
        "            for category, articles in categories.items():\n",
        "                # Score each article\n",
        "                for article in articles:\n",
        "                    score = compute_semantic_score( article['summary'])\n",
        "                    article['semantic_score'] = score\n",
        "                \n",
        "                # Sort articles by semantic score descending\n",
        "                articles.sort(key=lambda x: x['semantic_score'], reverse=True)\n",
        "    return data\n",
        "\n",
        "# 4. Apply the scoring and ranking\n",
        "ranked_news_data = rank_and_score_news(news_data)\n",
        "\n",
        "# 5. Save the final ranked JSON\n",
        "with open(\"companies_ranked_news.json\", \"w\") as f:\n",
        "    json.dump(ranked_news_data, f, indent=4)\n",
        "\n",
        "print(\"Scoring and ranking complete. Saved to companies_ranked_news.json\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
